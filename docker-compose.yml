name: prime-jennie

# ============================================================================
# 공통 앵커
# ============================================================================
x-service-defaults: &service-defaults
  build:
    context: .
    dockerfile: Dockerfile
  env_file: .env
  restart: unless-stopped
  network_mode: host
  labels:
    - stack=prime-jennie

x-env-common: &env-common
  DB_HOST: 127.0.0.1
  DB_PORT: "3307"
  DB_USER: prime
  DB_PASSWORD: "${DB_PASSWORD:-q1w2e3R$}"
  DB_NAME: prime_jennie
  REDIS_HOST: 127.0.0.1
  REDIS_PORT: "6379"
  KIS_GATEWAY_URL: http://127.0.0.1:8080
  APP_LOG_LEVEL: INFO
  APP_TIMEZONE: Asia/Seoul

# ============================================================================
# Infrastructure Services (profile: infra)
# ============================================================================
services:
  mariadb:
    image: mariadb:10.11
    profiles: ["infra"]
    environment:
      MARIADB_ROOT_PASSWORD: "${DB_PASSWORD:-q1w2e3R$}"
      MARIADB_USER: prime
      MARIADB_PASSWORD: "${DB_PASSWORD:-q1w2e3R$}"
      MARIADB_DATABASE: prime_jennie
    command: --lower_case_table_names=1
    ports:
      - "3307:3306"
    volumes:
      - /docker_data/mariadb_data:/var/lib/mysql
    restart: always
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 5
    network_mode: host
    labels:
      - app=mariadb
      - stack=prime-jennie

  redis:
    image: redis:alpine
    profiles: ["infra"]
    ports:
      - "6379:6379"
    volumes:
      - /docker_data/redis_data:/data
    restart: always
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    network_mode: host
    labels:
      - app=redis
      - stack=prime-jennie

  qdrant:
    image: qdrant/qdrant:latest
    profiles: ["infra"]
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - /docker_data/qdrant_data:/qdrant/storage
    restart: always
    environment:
      QDRANT__SERVICE__GRPC_PORT: "6334"
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'cat < /dev/tcp/localhost/6333'"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    network_mode: host
    labels:
      - app=qdrant
      - stack=prime-jennie

  # ============================================================================
  # Observability (profile: infra)
  # ============================================================================
  loki:
    image: grafana/loki:2.9.0
    profiles: ["infra"]
    command: -config.file=/etc/loki/loki-config.yaml
    volumes:
      - ./infra/loki/loki-config.yaml:/etc/loki/loki-config.yaml:ro
      - /docker_data/loki_data:/loki
    restart: unless-stopped
    network_mode: host
    labels:
      - app=loki
      - stack=prime-jennie

  grafana:
    image: grafana/grafana:10.4.0
    profiles: ["infra"]
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_PASSWORD:-admin}"
      GF_SERVER_HTTP_PORT: "3300"
    volumes:
      - /docker_data/grafana_data:/var/lib/grafana
      - ./infra/grafana/provisioning:/etc/grafana/provisioning:ro
    restart: unless-stopped
    network_mode: host
    labels:
      - app=grafana
      - stack=prime-jennie

  # ============================================================================
  # vLLM (profile: infra)
  # ============================================================================
  vllm-llm:
    image: vllm/vllm-openai:latest
    profiles: ["infra"]
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"
    command: >
      --model LGAI-EXAONE/EXAONE-4.0-32B-AWQ
      --port 8001
      --gpu-memory-utilization 0.90
      --max-model-len 4096
      --quantization awq
      --trust-remote-code
    ports:
      - "8001:8001"
    volumes:
      - /docker_data/vllm_cache:/root/.cache/huggingface
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s
    network_mode: host
    labels:
      - app=vllm-llm
      - stack=prime-jennie

  vllm-embed:
    image: vllm/vllm-openai:latest
    profiles: ["infra"]
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "0"
    command: >
      --model nlpai-lab/KURE-v1
      --port 8002
      --gpu-memory-utilization 0.05
      --max-model-len 512
      --trust-remote-code
    ports:
      - "8002:8002"
    volumes:
      - /docker_data/vllm_cache:/root/.cache/huggingface
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 51s
    network_mode: host
    labels:
      - app=vllm-embed
      - stack=prime-jennie

  # ============================================================================
  # Application Services (profile: real)
  # ============================================================================
  kis-gateway:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8080"
      KIS_APP_KEY: "${KIS_APP_KEY}"
      KIS_APP_SECRET: "${KIS_APP_SECRET}"
      KIS_ACCOUNT_NO: "${KIS_ACCOUNT_NO}"
      APP_TRADING_MODE: REAL
    command: ["uvicorn", "prime_jennie.services.gateway.app:app", "--host", "0.0.0.0", "--port", "8080"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    labels:
      - app=kis-gateway
      - stack=prime-jennie

  scout-job:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8087"
      SCORING_QUANT_SCORER_VERSION: v2
      SCORING_UNIFIED_ANALYST_ENABLED: "true"
      SCOUT_MAX_WATCHLIST_SIZE: "20"
      LLM_TIER_FAST_PROVIDER: ollama
      LLM_TIER_REASONING_PROVIDER: deepseek_cloud
      LLM_VLLM_LLM_URL: http://127.0.0.1:8001/v1
      LLM_VLLM_EMBED_URL: http://127.0.0.1:8002/v1
    command: ["uvicorn", "prime_jennie.services.scout.app:app", "--host", "0.0.0.0", "--port", "8087"]
    labels:
      - app=scout-job
      - stack=prime-jennie

  buy-scanner:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8081"
      SCANNER_CONVICTION_ENTRY_ENABLED: "true"
      SCANNER_MOMENTUM_LIMIT_ORDER_ENABLED: "true"
      SCANNER_MOMENTUM_CONFIRMATION_BARS: "1"
    command: ["uvicorn", "prime_jennie.services.scanner.app:app", "--host", "0.0.0.0", "--port", "8081"]
    labels:
      - app=buy-scanner
      - stack=prime-jennie

  buy-executor:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8082"
      RISK_PORTFOLIO_GUARD_ENABLED: "true"
      RISK_DYNAMIC_SECTOR_BUDGET_ENABLED: "true"
      RISK_MAX_PORTFOLIO_SIZE: "10"
      RISK_MAX_BUY_COUNT_PER_DAY: "6"
    command: ["uvicorn", "prime_jennie.services.buyer.app:app", "--host", "0.0.0.0", "--port", "8082"]
    labels:
      - app=buy-executor
      - stack=prime-jennie

  sell-executor:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8083"
    command: ["uvicorn", "prime_jennie.services.seller.app:app", "--host", "0.0.0.0", "--port", "8083"]
    labels:
      - app=sell-executor
      - stack=prime-jennie

  price-monitor:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8088"
      SELL_TRAILING_ENABLED: "true"
      SELL_TRAILING_ACTIVATION_PCT: "5.0"
      SELL_STOP_LOSS_PCT: "5.0"
    command: ["uvicorn", "prime_jennie.services.monitor.app:app", "--host", "0.0.0.0", "--port", "8088"]
    labels:
      - app=price-monitor
      - stack=prime-jennie

  macro-council:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8089"
      LLM_TIER_REASONING_PROVIDER: deepseek_cloud
      LLM_TIER_THINKING_PROVIDER: deepseek_cloud
    command: ["uvicorn", "prime_jennie.services.council.app:app", "--host", "0.0.0.0", "--port", "8089"]
    labels:
      - app=macro-council
      - stack=prime-jennie

  news-pipeline:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8092"
      LLM_TIER_FAST_PROVIDER: ollama
      LLM_VLLM_LLM_URL: http://127.0.0.1:8001/v1
      LLM_VLLM_EMBED_URL: http://127.0.0.1:8002/v1
      INFRA_QDRANT_URL: http://127.0.0.1:6333
    command: ["uvicorn", "prime_jennie.services.news.app:app", "--host", "0.0.0.0", "--port", "8092"]
    labels:
      - app=news-pipeline
      - stack=prime-jennie

  dashboard:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8090"
    command: ["uvicorn", "prime_jennie.services.dashboard.app:app", "--host", "0.0.0.0", "--port", "8090"]
    labels:
      - app=dashboard
      - stack=prime-jennie

  dashboard-frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    profiles: ["real"]
    restart: unless-stopped
    network_mode: host
    depends_on:
      - dashboard
    labels:
      - app=dashboard-frontend
      - stack=prime-jennie

  daily-briefing:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8086"
      LLM_TIER_FAST_PROVIDER: ollama
    command: ["uvicorn", "prime_jennie.services.briefing.app:app", "--host", "0.0.0.0", "--port", "8086"]
    labels:
      - app=daily-briefing
      - stack=prime-jennie

  telegram:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8091"
      TELEGRAM_BOT_TOKEN: "${TELEGRAM_BOT_TOKEN}"
      TELEGRAM_CHAT_IDS: "${TELEGRAM_CHAT_IDS}"
    command: ["uvicorn", "prime_jennie.services.telegram.app:app", "--host", "0.0.0.0", "--port", "8091"]
    labels:
      - app=telegram
      - stack=prime-jennie

  job-worker:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      PORT: "8095"
      LLM_TIER_FAST_PROVIDER: ollama
      LLM_VLLM_LLM_URL: http://127.0.0.1:8001/v1
      INFRA_QDRANT_URL: http://127.0.0.1:6333
    command: ["uvicorn", "prime_jennie.services.jobs.app:app", "--host", "0.0.0.0", "--port", "8095"]
    labels:
      - app=job-worker
      - stack=prime-jennie

  # ============================================================================
  # Airflow (profile: real)
  # ============================================================================
  airflow-webserver:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__DAGS_FOLDER: /app/dags
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "mysql+pymysql://prime:${DB_PASSWORD:-q1w2e3R$}@127.0.0.1:3307/prime_jennie"
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "8085"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_HOME: /app/airflow_home
      TELEGRAM_BOT_TOKEN: "${TELEGRAM_BOT_TOKEN}"
      TELEGRAM_CHAT_IDS: "${TELEGRAM_CHAT_IDS}"
    command: ["airflow", "webserver", "--port", "8085"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      - app=airflow-webserver
      - stack=prime-jennie

  airflow-scheduler:
    <<: *service-defaults
    profiles: ["real"]
    environment:
      <<: *env-common
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__DAGS_FOLDER: /app/dags
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "mysql+pymysql://prime:${DB_PASSWORD:-q1w2e3R$}@127.0.0.1:3307/prime_jennie"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_HOME: /app/airflow_home
    command: ["airflow", "scheduler"]
    labels:
      - app=airflow-scheduler
      - stack=prime-jennie
